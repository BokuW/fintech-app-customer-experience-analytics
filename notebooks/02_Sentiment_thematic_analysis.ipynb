{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0231b6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #notebooks/02_sentiment_thematic_analysis.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re # For regex in text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0419ddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-2\\fintech-app-customer-experience-analytics' already in sys.path.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Project Setup: Ensure src module is discoverable ---\n",
    "def find_project_root(current_path):\n",
    "    path = current_path\n",
    "    while path != os.path.dirname(path):\n",
    "        if (os.path.isdir(os.path.join(path, 'src')) and\n",
    "            os.path.isdir(os.path.join(path, 'data')) and\n",
    "            os.path.isdir(os.path.join(path, 'notebooks'))):\n",
    "            return path\n",
    "        path = os.path.dirname(path)\n",
    "    return current_path\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "project_root = find_project_root(current_working_dir)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path for module imports.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' already in sys.path.\")\n",
    "\n",
    "# Import configuration variables\n",
    "from src.config import CLEAN_REVIEWS_CSV, PROCESSED_DATA_DIR\n",
    "\n",
    "# Define output file for Task 2 results\n",
    "SENTIMENT_THEMES_CSV = os.path.join(PROCESSED_DATA_DIR, 'reviews_with_sentiment_themes.csv')\n",
    "# Ensure the output directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "266d2521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Cleaned Review Data ---\n",
      "Cleaned data loaded successfully from c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-2\\fintech-app-customer-experience-analytics\\data\\processed\\clean_play_store_reviews.csv.\n",
      "Initial DataFrame shape: (8989, 6)\n",
      "\n",
      "First 5 rows of loaded data:\n",
      "                              review_id  \\\n",
      "0  a7d1c799-ba53-4a0a-a8d6-c5400a009825   \n",
      "1  64ed5562-1758-4eb8-9291-8b6edc394118   \n",
      "2  d0c05687-ddd4-43fb-95a9-08f6358d80a2   \n",
      "3  811bf820-3529-433a-9b6d-e624fa23a16a   \n",
      "4  be2cb2ac-bbe0-4175-81c4-9f6c86afdaaa   \n",
      "\n",
      "                                         review_text  rating        date  \\\n",
      "0  A great app. It's like carrying a bank in your...       4  2025-06-07   \n",
      "1                      More than garrantty bank EBC.       4  2025-06-07   \n",
      "2  really am happy to this app it is Siple to use...       5  2025-06-07   \n",
      "3  I liked this app. But the User interface is ve...       2  2025-06-07   \n",
      "4  \"Why donâ€™t your ATMs support account-to-accoun...       4  2025-06-06   \n",
      "\n",
      "                     bank_name       source  \n",
      "0  Commercial Bank of Ethiopia  Google Play  \n",
      "1  Commercial Bank of Ethiopia  Google Play  \n",
      "2  Commercial Bank of Ethiopia  Google Play  \n",
      "3  Commercial Bank of Ethiopia  Google Play  \n",
      "4  Commercial Bank of Ethiopia  Google Play  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Step 1: Load Cleaned Data ---\n",
    "print(\"\\n--- Loading Cleaned Review Data ---\")\n",
    "try:\n",
    "    df = pd.read_csv(CLEAN_REVIEWS_CSV)\n",
    "    print(f\"Cleaned data loaded successfully from {CLEAN_REVIEWS_CSV}.\")\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows of loaded data:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"CRITICAL ERROR: Cleaned data file not found at {CLEAN_REVIEWS_CSV}. Please run Task 1 first.\")\n",
    "    sys.exit(\"Exiting: Cleaned data not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not load cleaned data: {e}\")\n",
    "    sys.exit(\"Exiting: Cleaned data loading failed.\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"WARNING: Loaded DataFrame is empty. Skipping sentiment and thematic analysis.\")\n",
    "    sys.exit(\"Exiting: Empty DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Step 2: NLP Preprocessing for Sentiment and Thematic Analysis ---\n",
    "print(\"\\n--- Performing NLP Preprocessing (Tokenization, Stopword Removal) ---\")\n",
    "\n",
    "# Download NLTK data\n",
    "# These commands will download the necessary data if not already present.\n",
    "# It's more robust to call nltk.download() directly without complex try-except for DownloadError\n",
    "print(\"Downloading NLTK data (stopwords, punkt, vader_lexicon)...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "print(\"NLTK data download complete.\")\n",
    "\n",
    "# Initialize NLTK Stopwords and VADER Sentiment Analyzer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function for basic text cleaning (for TF-IDF and VADER)\n",
    "def clean_text_nlp(text):\n",
    "    text = str(text).lower() # Convert to string and lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
    "    text = re.sub(r'\\@w+|\\#', '', text) # Remove @mentions and hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply NLP cleaning\n",
    "df['cleaned_review_text'] = df['review_text'].apply(clean_text_nlp)\n",
    "\n",
    "# Check for reviews that might have become empty after cleaning\n",
    "empty_cleaned_reviews = df[df['cleaned_review_text'].str.strip() == '']\n",
    "if not empty_cleaned_reviews.empty:\n",
    "    print(f\"WARNING: {len(empty_cleaned_reviews)} reviews became empty after NLP cleaning. They might affect analysis.\")\n",
    "    # Option: You could fill these with a placeholder or drop them if they are too many.\n",
    "    # For now, we'll keep them but be aware.\n",
    "\n",
    "print(\"\\nFirst 5 rows of data with cleaned_review_text:\")\n",
    "print(df[['review_text', 'cleaned_review_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54388920",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Step 3: Sentiment Analysis using VADER ---\n",
    "print(\"\\n--- Performing Sentiment Analysis (VADER) ---\")\n",
    "\n",
    "# Function to get VADER sentiment scores\n",
    "def get_vader_sentiment(text):\n",
    "    if not isinstance(text, str) or not text.strip(): # Handle non-string or empty texts\n",
    "        return {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} # Neutral score for empty\n",
    "    vs = analyzer.polarity_scores(text)\n",
    "    return vs\n",
    "\n",
    "# Apply sentiment analysis\n",
    "# Using .progress_apply for a progress bar as it can take time\n",
    "df['sentiment_scores'] = df['cleaned_review_text'].apply(get_vader_sentiment)\n",
    "\n",
    "# Extract individual sentiment scores\n",
    "df['sentiment_neg'] = df['sentiment_scores'].apply(lambda x: x['neg'])\n",
    "df['sentiment_neu'] = df['sentiment_scores'].apply(lambda x: x['neu'])\n",
    "df['sentiment_pos'] = df['sentiment_scores'].apply(lambda x: x['pos'])\n",
    "df['sentiment_compound'] = df['sentiment_scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Determine sentiment label (positive, negative, neutral) based on compound score\n",
    "def get_sentiment_label(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "df['sentiment_label'] = df['sentiment_compound'].apply(get_sentiment_label)\n",
    "\n",
    "print(\"\\nSentiment analysis completed. First 5 rows with sentiment scores and labels:\")\n",
    "print(df[['review_text', 'sentiment_compound', 'sentiment_label']].head())\n",
    "print(\"\\nSentiment label distribution:\")\n",
    "print(df['sentiment_label'].value_counts())\n",
    "\n",
    "# KPI Check: Sentiment scores for 90%+ reviews.\n",
    "# Since we explicitly handled empty strings and assigned neutral, this should be 100% non-null.\n",
    "sentiment_coverage = (df['sentiment_compound'].notnull().sum() / len(df)) * 100\n",
    "print(f\"\\nSentiment coverage: {sentiment_coverage:.2f}%\")\n",
    "if sentiment_coverage >= 90:\n",
    "    print(\"KPI Met: Sentiment scores for 90%+ reviews.\")\n",
    "else:\n",
    "    print(\"KPI Warning: Sentiment coverage is below 90%.\")\n",
    "\n",
    "\n",
    "# --- Step 4: Thematic Analysis (Keyword Extraction using TF-IDF) ---\n",
    "print(\"\\n--- Performing Thematic Analysis (Keyword Extraction) ---\")\n",
    "\n",
    "# Combine reviews by bank for TF-IDF\n",
    "bank_reviews_combined = df.groupby('bank_name')['cleaned_review_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "# max_df can be used to ignore terms that appear in too many documents (e.g., 85% of reviews)\n",
    "# min_df can be used to ignore terms that appear in too few documents (e.g., less than 5 documents)\n",
    "# ngram_range to get single words and common phrases\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.85, min_df=5, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform for each bank (separately or combined, let's do combined for general keywords)\n",
    "# For bank-specific keywords, we might want to run TFIDF per bank, but for now, let's get general keywords\n",
    "# and then focus on manual clustering per bank later.\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(bank_reviews_combined['cleaned_review_text'])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nTop 10 TF-IDF keywords (Unigrams & Bigrams) per bank:\")\n",
    "bank_keywords = {}\n",
    "for i, row in bank_reviews_combined.iterrows():\n",
    "    bank_name = row['bank_name']\n",
    "    \n",
    "    # Get TF-IDF scores for the current bank\n",
    "    vector = tfidf_matrix[i]\n",
    "    # Create a DataFrame of feature names and their TF-IDF scores for the current bank\n",
    "    tfidf_scores = pd.DataFrame(vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "    tfidf_scores = tfidf_scores.sort_values(by=\"tfidf\", ascending=False)\n",
    "    \n",
    "    bank_keywords[bank_name] = tfidf_scores.head(20).index.tolist() # Top 20 keywords for each bank\n",
    "    \n",
    "    print(f\"\\n--- {bank_name} ---\")\n",
    "    print(bank_keywords[bank_name])\n",
    "\n",
    "# --- Step 5: Manual/Rule-Based Thematic Clustering ---\n",
    "# This is a conceptual step that you would refine based on the keywords above.\n",
    "# Here, we'll demonstrate a simple rule-based approach for common themes.\n",
    "# You will likely need to refine these rules significantly based on your actual data.\n",
    "\n",
    "def assign_theme(review_text, bank_name):\n",
    "    text = str(review_text).lower() # Ensure it's lowercase for matching\n",
    "    \n",
    "    # Define keywords for common themes\n",
    "    # These are examples, you would refine these based on your TF-IDF results\n",
    "    themes = {\n",
    "        'Account Access Issues': ['login', 'account', 'password', 'face id', 'fingerprint'],\n",
    "        'Transaction Performance': ['transfer', 'send', 'receive', 'transaction', 'slow', 'fast', 'delay'],\n",
    "        'User Interface & Experience': ['ui', 'interface', 'design', 'easy', 'user friendly', 'layout', 'bug', 'crashes', 'update'],\n",
    "        'Customer Support': ['support', 'customer service', 'help', 'call', 'response'],\n",
    "        'Feature Requests': ['budgeting', 'new feature', 'qr', 'international', 'bill pay', 'online banking']\n",
    "    }\n",
    "    \n",
    "    identified_themes = []\n",
    "    for theme, keywords in themes.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            identified_themes.append(theme)\n",
    "            \n",
    "    if not identified_themes:\n",
    "        return 'Other/General Feedback' # Default theme if no specific keywords match\n",
    "    return ', '.join(identified_themes) # Join multiple themes if applicable\n",
    "\n",
    "print(\"\\n--- Assigning Themes to Reviews ---\")\n",
    "df['identified_themes'] = df['cleaned_review_text'].apply(lambda x: assign_theme(x, x)) # bank_name not used in this simplified func\n",
    "\n",
    "print(\"\\nThemes assigned. First 5 rows with identified_themes:\")\n",
    "print(df[['review_text', 'identified_themes']].head())\n",
    "\n",
    "# KPI Check: 3+ themes per bank with examples (examples will be seen in your manual review of themes)\n",
    "# This check confirms that the theme assignment process is working, and themes exist.\n",
    "print(\"\\nTheme distribution by bank:\")\n",
    "theme_counts_by_bank = df.groupby('bank_name')['identified_themes'].value_counts().unstack(fill_value=0)\n",
    "print(theme_counts_by_bank)\n",
    "\n",
    "for bank in df['bank_name'].unique():\n",
    "    num_themes = len(df[df['bank_name'] == bank]['identified_themes'].unique())\n",
    "    if num_themes >= 3:\n",
    "        print(f\"KPI Met for {bank}: {num_themes} themes identified.\")\n",
    "    else:\n",
    "        print(f\"KPI Warning for {bank}: Only {num_themes} themes identified.\")\n",
    "\n",
    "\n",
    "# --- Step 6: Aggregate Sentiment by Bank and Rating ---\n",
    "print(\"\\n--- Aggregating Sentiment by Bank and Rating ---\")\n",
    "\n",
    "# Aggregate by bank and rating\n",
    "sentiment_by_bank_rating = df.groupby(['bank_name', 'rating'])['sentiment_compound'].mean().unstack(fill_value=0)\n",
    "print(\"\\nAverage Compound Sentiment Score by Bank and Rating:\")\n",
    "print(sentiment_by_bank_rating)\n",
    "\n",
    "# Aggregate overall sentiment per bank\n",
    "overall_sentiment_by_bank = df.groupby('bank_name')['sentiment_compound'].mean().reset_index()\n",
    "print(\"\\nOverall Average Compound Sentiment Score by Bank:\")\n",
    "print(overall_sentiment_by_bank)\n",
    "\n",
    "# Aggregate sentiment label counts per bank\n",
    "sentiment_label_counts = df.groupby(['bank_name', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "print(\"\\nSentiment Label Counts by Bank:\")\n",
    "print(sentiment_label_counts)\n",
    "\n",
    "\n",
    "# --- Step 7: Save Results ---\n",
    "print(f\"\\n--- Saving Processed Reviews with Sentiment and Themes to: {SENTIMENT_THEMES_CSV} ---\")\n",
    "\n",
    "# Select columns as specified in deliverables: review_id, review_text, sentiment_label, sentiment_score, identified_theme(s)\n",
    "# Note: 'sentiment_score' is the compound score here.\n",
    "final_output_df = df[['review_id', 'review_text', 'sentiment_label', 'sentiment_compound', 'identified_themes', 'bank_name', 'rating', 'date', 'source']]\n",
    "\n",
    "final_output_df.to_csv(SENTIMENT_THEMES_CSV, index=False)\n",
    "print(\"File saved successfully.\")\n",
    "\n",
    "print(\"\\n--- Task 2: Sentiment and Thematic Analysis Complete ---\")\n",
    "print(\"Remember to commit your work to your 'task-2' branch!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
