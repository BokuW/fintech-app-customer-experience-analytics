{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0231b6",
   "metadata": {},
   "outputs": [],
   "source": [
    " #notebooks/02_sentiment_thematic_analysis.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re # For regex in text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0419ddaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-2\\fintech-app-customer-experience-analytics' to sys.path for module imports.\n",
      "Project structure setup complete and config.py created/updated.\n",
      "Base Directory: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-2\\fintech-app-customer-experience-analytics\n",
      "Raw Data Directory: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-2\\fintech-app-customer-experience-analytics\\data\\raw\n",
      "Processed Data Directory: c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-2\\fintech-app-customer-experience-analytics\\data\\processed\n",
      "App IDs to scrape: {'Commercial Bank of Ethiopia': 'com.combanketh.mobilebanking', 'Bank of Abyssinia': 'com.boa.boaMobileBanking', 'Dashen Bank': 'com.dashen.dashensuperapp'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Project Setup: Ensure src module is discoverable ---\n",
    "def find_project_root(current_path):\n",
    "    path = current_path\n",
    "    while path != os.path.dirname(path):\n",
    "        if (os.path.isdir(os.path.join(path, 'src')) and\n",
    "            os.path.isdir(os.path.join(path, 'data')) and\n",
    "            os.path.isdir(os.path.join(path, 'notebooks'))):\n",
    "            return path\n",
    "        path = os.path.dirname(path)\n",
    "    return current_path\n",
    "\n",
    "current_working_dir = os.getcwd()\n",
    "project_root = find_project_root(current_working_dir)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added '{project_root}' to sys.path for module imports.\")\n",
    "else:\n",
    "    print(f\"'{project_root}' already in sys.path.\")\n",
    "\n",
    "# Import configuration variables\n",
    "from src.config import CLEAN_REVIEWS_CSV, PROCESSED_DATA_DIR\n",
    "\n",
    "# Define output file for Task 2 results\n",
    "SENTIMENT_THEMES_CSV = os.path.join(PROCESSED_DATA_DIR, 'reviews_with_sentiment_themes.csv')\n",
    "# Ensure the output directory exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "266d2521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Cleaned Review Data ---\n",
      "Cleaned data loaded successfully from c:\\Users\\hp\\OneDrive\\Desktop\\kaim-ai\\KAIM-2\\fintech-app-customer-experience-analytics\\data\\processed\\clean_play_store_reviews.csv.\n",
      "Initial DataFrame shape: (8989, 6)\n",
      "\n",
      "First 5 rows of loaded data:\n",
      "                              review_id  \\\n",
      "0  a7d1c799-ba53-4a0a-a8d6-c5400a009825   \n",
      "1  64ed5562-1758-4eb8-9291-8b6edc394118   \n",
      "2  d0c05687-ddd4-43fb-95a9-08f6358d80a2   \n",
      "3  811bf820-3529-433a-9b6d-e624fa23a16a   \n",
      "4  be2cb2ac-bbe0-4175-81c4-9f6c86afdaaa   \n",
      "\n",
      "                                         review_text  rating        date  \\\n",
      "0  A great app. It's like carrying a bank in your...       4  2025-06-07   \n",
      "1                      More than garrantty bank EBC.       4  2025-06-07   \n",
      "2  really am happy to this app it is Siple to use...       5  2025-06-07   \n",
      "3  I liked this app. But the User interface is ve...       2  2025-06-07   \n",
      "4  \"Why don’t your ATMs support account-to-accoun...       4  2025-06-06   \n",
      "\n",
      "                     bank_name       source  \n",
      "0  Commercial Bank of Ethiopia  Google Play  \n",
      "1  Commercial Bank of Ethiopia  Google Play  \n",
      "2  Commercial Bank of Ethiopia  Google Play  \n",
      "3  Commercial Bank of Ethiopia  Google Play  \n",
      "4  Commercial Bank of Ethiopia  Google Play  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 1: Load Cleaned Data ---\n",
    "print(\"\\n--- Loading Cleaned Review Data ---\")\n",
    "try:\n",
    "    df = pd.read_csv(CLEAN_REVIEWS_CSV)\n",
    "    print(f\"Cleaned data loaded successfully from {CLEAN_REVIEWS_CSV}.\")\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows of loaded data:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"CRITICAL ERROR: Cleaned data file not found at {CLEAN_REVIEWS_CSV}. Please run Task 1 first.\")\n",
    "    sys.exit(\"Exiting: Cleaned data not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not load cleaned data: {e}\")\n",
    "    sys.exit(\"Exiting: Cleaned data loading failed.\")\n",
    "\n",
    "if df.empty:\n",
    "    print(\"WARNING: Loaded DataFrame is empty. Skipping sentiment and thematic analysis.\")\n",
    "    sys.exit(\"Exiting: Empty DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e29473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing NLP Preprocessing (Tokenization, Stopword Removal) ---\n",
      "Downloading NLTK data (stopwords, punkt, vader_lexicon)...\n",
      "NLTK data download complete.\n",
      "WARNING: 124 reviews became empty after NLP cleaning. They might affect analysis.\n",
      "\n",
      "First 5 rows of data with cleaned_review_text:\n",
      "                                         review_text  \\\n",
      "0  A great app. It's like carrying a bank in your...   \n",
      "1                      More than garrantty bank EBC.   \n",
      "2  really am happy to this app it is Siple to use...   \n",
      "3  I liked this app. But the User interface is ve...   \n",
      "4  \"Why don’t your ATMs support account-to-accoun...   \n",
      "\n",
      "                                 cleaned_review_text  \n",
      "0  a great app its like carrying a bank in your p...  \n",
      "1                       more than garrantty bank ebc  \n",
      "2  really am happy to this app it is siple to use...  \n",
      "3  i liked this app but the user interface is ver...  \n",
      "4  why dont your atms support accounttoaccount tr...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 2: NLP Preprocessing for Sentiment and Thematic Analysis ---\n",
    "print(\"\\n--- Performing NLP Preprocessing (Tokenization, Stopword Removal) ---\")\n",
    "\n",
    "# Download NLTK data\n",
    "# These commands will download the necessary data if not already present.\n",
    "# It's more robust to call nltk.download() directly without complex try-except for DownloadError\n",
    "print(\"Downloading NLTK data (stopwords, punkt, vader_lexicon)...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "print(\"NLTK data download complete.\")\n",
    "\n",
    "# Initialize NLTK Stopwords and VADER Sentiment Analyzer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function for basic text cleaning (for TF-IDF and VADER)\n",
    "def clean_text_nlp(text):\n",
    "    text = str(text).lower() # Convert to string and lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # Remove URLs\n",
    "    text = re.sub(r'\\@w+|\\#', '', text) # Remove @mentions and hashtags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply NLP cleaning\n",
    "df['cleaned_review_text'] = df['review_text'].apply(clean_text_nlp)\n",
    "\n",
    "# Check for reviews that might have become empty after cleaning\n",
    "empty_cleaned_reviews = df[df['cleaned_review_text'].str.strip() == '']\n",
    "if not empty_cleaned_reviews.empty:\n",
    "    print(f\"WARNING: {len(empty_cleaned_reviews)} reviews became empty after NLP cleaning. They might affect analysis.\")\n",
    "    # Option: You could fill these with a placeholder or drop them if they are too many.\n",
    "    # For now, we'll keep them but be aware.\n",
    "\n",
    "print(\"\\nFirst 5 rows of data with cleaned_review_text:\")\n",
    "print(df[['review_text', 'cleaned_review_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54388920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
